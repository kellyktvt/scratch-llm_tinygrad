{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METAL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad import Tensor, dtypes, Device\n",
    "\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer, train_tokenizer\n",
    "\n",
    "from helpers.dataset import NextTokenPredictionDataset\n",
    "from helpers.trainer import train\n",
    "from helpers.config import LLMConfig, TrainingConfig\n",
    "from helpers.dataloader import DataLoader, default_collate\n",
    "\n",
    "print(Device.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies achitecture and hyperparameters of the language model\n",
    "llm_config = LLMConfig(\n",
    "    # size of the vocab the model can understand\n",
    "    vocab_size=4096,\n",
    "    # max sequence length of the input tokens; # of tokens model can process in 1 forward pass\n",
    "    seq_len=128,\n",
    "    # dimensionality of embedding vectors; each token in vocab is repped by vector of this size\n",
    "    dim_emb=256,\n",
    "    # num of layers (or transformer blocks) in the model; each consists of sub-layers like self-attention and feedforward\n",
    "    num_layers=4,\n",
    "    # num of attention heads in the multi-head attention mechanism\n",
    "    num_heads=8,\n",
    "    # dropout rate applied to embedding layer to prevent overfitting\n",
    "    emb_dropout=0.0,\n",
    "    # dimensionality of hidden layer in feedforward network; typically a multiple of 'dim-emb'\n",
    "    ffn_dim_hidden=4 * 256,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=False,\n",
    ")\n",
    "\n",
    "# specifies parameters and settings for training the language model\n",
    "train_config = TrainingConfig(\n",
    "    # whether to retrain the tokenizer or not \n",
    "    retrain_tokenizer=False,\n",
    "    # num of samples per batch of training\n",
    "    batch_size=64,\n",
    "    # learning rate for the optimizer; controls how much to adjust the weights w/ respect to the loss gradient\n",
    "    learning_rate=3e-4,\n",
    "    # weight decay parameter; helps prevent overfitting by penalizing large weights\n",
    "    weight_decay=1e-5,\n",
    "    # max number of epochs (full passes thru training dataset) to train the model for\n",
    "    max_epochs=1,\n",
    "    # freq of logging training process (1 = logging after every batch/epoch)\n",
    "    log_frequency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tokenizer and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies path to input text file used for training or retraining tokenizer\n",
    "input_file = \"../data/shakespeare/tinyshakespeare.txt\"\n",
    "# creates new file path for tokenizer model by changing suffix of input file\n",
    "output_file = Path(input_file).with_suffix(\".model\")\n",
    "\n",
    "# checks whether tokenizer model file alr exists or if config specifies to retrain tokenizer\n",
    "if not output_file.exists() or train_config.retrain_tokenizer:\n",
    "    # train tokenizer sing input text file and specified vocab size from 'LLMConfig'\n",
    "    train_tokenizer(input_file, llm_config.vocab_size)\n",
    "\n",
    "# initialize tokenizer by loading it from 'output_file'\n",
    "tokenizer = Tokenizer(str(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Before', '▁we', '▁proceed', '▁any', '▁further', ',', '▁hear', '▁me', '▁speak', '.']\n"
     ]
    }
   ],
   "source": [
    "# defines a string 'sentence' that will be tokenized\n",
    "sentence = \"Before we proceed any further, hear me speak.\"\n",
    "# uses 'EncodeAsPieces' method from 'tokenizer.sp' object to tokenize sentence into tokens and outputs them\n",
    "print(tokenizer.sp.EncodeAsPieces(sentence))\n",
    "\n",
    "# ensures that the encoding and decoding returns the original sentence\n",
    "assert tokenizer.decode(tokenizer.encode(sentence)) == sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128) (64, 128)\n"
     ]
    }
   ],
   "source": [
    "# This helper class allow to generate batches of inputs and targets where targets last element is the next token to predict\n",
    "\n",
    "# initializes instance of 'NextTokenPredictionDataset' class; dataset responsible for generating batches of input sequences and corresponding target sequences for training\n",
    "ds_train = NextTokenPredictionDataset(input_file, llm_config.seq_len, tokenizer)\n",
    "# initializes 'Dataloader' instance that's responsible for creating batches of data from the dataset\n",
    "dl_train = DataLoader(ds_train, batch_size=train_config.batch_size, shuffle=True)\n",
    "\n",
    "# iterates over the batches generates by the data loader\n",
    "for inputs, labels in dl_train:\n",
    "    # prints shapes of 'inputs' and 'labels' tensors; helps verify dimensions of batches\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes instance of 'LLM' class; represents the language model\n",
    "model = LLM(\n",
    "    # size of the vocabulary (# of unique tokens)\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    # max sequence length (# of tokens per input sequence)\n",
    "    seq_len=llm_config.seq_len,\n",
    "    # dimensionality of embeddings vectors\n",
    "    dim_emb=llm_config.dim_emb,\n",
    "    # number of layers (transformer block) in the model\n",
    "    num_layers=llm_config.num_layers,\n",
    "    # number of attention heads in the multi-head attention mechanism\n",
    "    attn_num_heads=llm_config.num_heads,\n",
    "    # dropout rate applied to the embedding layer\n",
    "    emb_dropout=llm_config.emb_dropout,\n",
    "    # dimensionality of the hidden layer in the feedforward network\n",
    "    ffn_hidden_dim=llm_config.ffn_dim_hidden,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=llm_config.ffn_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|              token_embedding.weight             |  1048576   |\n",
      "|           transformer.0.norm_attn.gain          |    256     |\n",
      "|   transformer.0.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.0.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.0.norm_ffn.gain           |    256     |\n",
      "|    transformer.0.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.0.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.0.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.0.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.1.norm_attn.gain          |    256     |\n",
      "|   transformer.1.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.1.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.1.norm_ffn.gain           |    256     |\n",
      "|    transformer.1.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.1.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.1.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.1.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.2.norm_attn.gain          |    256     |\n",
      "|   transformer.2.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.2.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.2.norm_ffn.gain           |    256     |\n",
      "|    transformer.2.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.2.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.2.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.2.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.3.norm_attn.gain          |    256     |\n",
      "|   transformer.3.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.3.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.3.norm_ffn.gain           |    256     |\n",
      "|    transformer.3.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.3.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.3.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.3.feed_forward.linear2.weight    |   262144   |\n",
      "|                    norm.gain                    |    256     |\n",
      "|               projection_head.bias              |    4096    |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 12597504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(12597504)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    # Token embedding\n",
    "    params = np.prod(model.token_embedding.weight.shape)\n",
    "    table.add_row([\"token_embedding.weight\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Transformer blocks\n",
    "    for i, block in enumerate(model.transformer_blocks):\n",
    "        # Attention norm\n",
    "        params = np.prod(block.norm_attn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_attn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Multi-head attention\n",
    "        params = np.prod(block.multihead_attn.proj_qkv.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_qkv.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.multihead_attn.proj_out.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_out.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # FFN norm\n",
    "        params = np.prod(block.norm_ffn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_ffn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Feed forward layers\n",
    "        params = np.prod(block.feed_forward.linear1.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear1.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.bias.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.bias\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.linear2.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear2.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "    # Final norm\n",
    "    params = np.prod(model.norm.gain.shape)\n",
    "    table.add_row([\"norm.gain\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Projection head\n",
    "    params = np.prod(model.projection_head.bias.shape)\n",
    "    table.add_row([\"projection_head.bias\", params])\n",
    "    total_params += params\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainingConfig' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calls 'train' fxn to train the language model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# language model instance to be trained\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# data loader that provides batches of training data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dl_train,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# device on which to train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# learning rate for the optimizer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     lr\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# max num of epochs to train the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# weight decay parameter to prevent overfitting\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mweight_decay,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# frequency of logging training progress\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     log_every\u001b[38;5;241m=\u001b[39mtrain_config\u001b[38;5;241m.\u001b[39mlog_frequency,\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingConfig' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# calls 'train' fxn to train the language model\n",
    "loss_history = train(\n",
    "    # language model instance to be trained\n",
    "    model,\n",
    "    # data loader that provides batches of training data\n",
    "    dl_train,\n",
    "    # learning rate for the optimizer\n",
    "    lr=train_config.learning_rate,\n",
    "    # max num of epochs to train the model\n",
    "    max_epochs=train_config.max_epochs,\n",
    "    # weight decay parameter to prevent overfitting\n",
    "    weight_decay=train_config.weight_decay,\n",
    "    # frequency of logging training progress\n",
    "    log_every=train_config.log_frequency,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new figure and axis object for the plot and sets the size of the figure\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# line plots training loss over time\n",
    "# x-axis: seq of ints from 0 to len of 'train_loss' list minus 1\n",
    "# y-axis: recorded training loss values\n",
    "ax.plot(range(len(loss_history[\"train_loss\"])), loss_history[\"train_loss\"])\n",
    "# sets label for the x-axis \n",
    "ax.set_xlabel(\"step\")\n",
    "# sets label for the y-axis\n",
    "ax.set_ylabel(\"cross entropy loss\")\n",
    "# adds horizontal grid lines to the plot, making it easier to read the y-axis values\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty prompt to generate random stuff\n",
    "# create 2D tensor w/ 1 row and 'llm_config.seq_len' cols, filled w/ the end-of-seq token ID and containing 32-bit ints\n",
    "prompt = torch.full((1, llm_config.seq_len), tokenizer.eos_id, dtype=torch.int32)\n",
    "# moves tensor to specified device for efficient computation\n",
    "prompt = prompt.to(train_config.device)\n",
    "\n",
    "# generates seq of tokens using the model starting from the 'prompt'\n",
    "out = model.generate(prompt, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from a prompt\n",
    "# encodes string into seq of token IDs using the tokenizer\n",
    "prompt = tokenizer.encode(\n",
    "    # prompt text to be encoded \n",
    "    \"KING HENRY VI:\",\n",
    "    # indicates this is the beginning of the string\n",
    "    beg_of_string=True,\n",
    "    # pads the sequence to the specified length\n",
    "    pad_seq=True,\n",
    "    # length to which the sequence should be padded or truncated\n",
    "    seq_len=llm_config.seq_len,\n",
    ")\n",
    "# converts encoded prompt to a PyTorch tensor and moves it to the specified device\n",
    "# 'torch.tensor(prompt, dtype=torch.int32)' converts prompt to a tensor w/ 32-bit int type\n",
    "# '.unsqueeze(0)' adds extra dimension at the beginning, making the tensor shape '(1, seq_len)'\n",
    "# '.to(train_config.device)' moves the tensor to the specified device\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0).to(train_config.device)\n",
    "# generates seq of tokens starting from the given prompt\n",
    "out = model.generate(inputs, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
