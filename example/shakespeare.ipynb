{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METAL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad import Tensor, dtypes, Device\n",
    "\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer, train_tokenizer\n",
    "\n",
    "from helpers.dataset import NextTokenPredictionDataset\n",
    "from helpers.trainer import train\n",
    "from helpers.config import LLMConfig, TrainingConfig\n",
    "from helpers.dataloader import DataLoader\n",
    "\n",
    "print(Device.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies achitecture and hyperparameters of the language model\n",
    "llm_config = LLMConfig(\n",
    "    # size of the vocab the model can understand\n",
    "    vocab_size=4096,\n",
    "    # max sequence length of the input tokens; # of tokens model can process in 1 forward pass\n",
    "    seq_len=128,\n",
    "    # dimensionality of embedding vectors; each token in vocab is repped by vector of this size\n",
    "    dim_emb=256,\n",
    "    # num of layers (or transformer blocks) in the model; each consists of sub-layers like self-attention and feedforward\n",
    "    num_layers=4,\n",
    "    # num of attention heads in the multi-head attention mechanism\n",
    "    num_heads=8,\n",
    "    # dropout rate applied to embedding layer to prevent overfitting\n",
    "    emb_dropout=0.0,\n",
    "    # dimensionality of hidden layer in feedforward network; typically a multiple of 'dim-emb'\n",
    "    ffn_dim_hidden=4 * 256,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=False,\n",
    ")\n",
    "\n",
    "# specifies parameters and settings for training the language model\n",
    "train_config = TrainingConfig(\n",
    "    # whether to retrain the tokenizer or not \n",
    "    retrain_tokenizer=False,\n",
    "    # num of samples per batch of training\n",
    "    batch_size=64,\n",
    "    # learning rate for the optimizer; controls how much to adjust the weights w/ respect to the loss gradient\n",
    "    learning_rate=3e-4,\n",
    "    # weight decay parameter; helps prevent overfitting by penalizing large weights\n",
    "    weight_decay=1e-5,\n",
    "    # max number of epochs (full passes thru training dataset) to train the model for\n",
    "    max_epochs=1,\n",
    "    # freq of logging training process (1 = logging after every batch/epoch)\n",
    "    log_frequency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tokenizer and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies path to input text file used for training or retraining tokenizer\n",
    "input_file = \"../data/shakespeare/tinyshakespeare.txt\"\n",
    "# creates new file path for tokenizer model by changing suffix of input file\n",
    "output_file = Path(input_file).with_suffix(\".model\")\n",
    "\n",
    "# checks whether tokenizer model file alr exists or if config specifies to retrain tokenizer\n",
    "if not output_file.exists() or train_config.retrain_tokenizer:\n",
    "    # train tokenizer sing input text file and specified vocab size from 'LLMConfig'\n",
    "    train_tokenizer(input_file, llm_config.vocab_size)\n",
    "\n",
    "# initialize tokenizer by loading it from 'output_file'\n",
    "tokenizer = Tokenizer(str(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Before', '▁we', '▁proceed', '▁any', '▁further', ',', '▁hear', '▁me', '▁speak', '.']\n"
     ]
    }
   ],
   "source": [
    "# defines a string 'sentence' that will be tokenized\n",
    "sentence = \"Before we proceed any further, hear me speak.\"\n",
    "# uses 'EncodeAsPieces' method from 'tokenizer.sp' object to tokenize sentence into tokens and outputs them\n",
    "print(tokenizer.sp.EncodeAsPieces(sentence))\n",
    "\n",
    "# ensures that the encoding and decoding returns the original sentence\n",
    "assert tokenizer.decode(tokenizer.encode(sentence)) == sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128) (64, 128)\n"
     ]
    }
   ],
   "source": [
    "# This helper class allow to generate batches of inputs and targets where targets last element is the next token to predict\n",
    "\n",
    "# initializes instance of 'NextTokenPredictionDataset' class; dataset responsible for generating batches of input sequences and corresponding target sequences for training\n",
    "ds_train = NextTokenPredictionDataset(input_file, llm_config.seq_len, tokenizer)\n",
    "# initializes 'Dataloader' instance that's responsible for creating batches of data from the dataset\n",
    "dl_train = DataLoader(ds_train, batch_size=train_config.batch_size, shuffle=True)\n",
    "\n",
    "# iterates over the batches generates by the data loader\n",
    "for inputs, labels in dl_train:\n",
    "    # prints shapes of 'inputs' and 'labels' tensors; helps verify dimensions of batches\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializes instance of 'LLM' class; represents the language model\n",
    "model = LLM(\n",
    "    # size of the vocabulary (# of unique tokens)\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    # max sequence length (# of tokens per input sequence)\n",
    "    seq_len=llm_config.seq_len,\n",
    "    # dimensionality of embeddings vectors\n",
    "    dim_emb=llm_config.dim_emb,\n",
    "    # number of layers (transformer block) in the model\n",
    "    num_layers=llm_config.num_layers,\n",
    "    # number of attention heads in the multi-head attention mechanism\n",
    "    attn_num_heads=llm_config.num_heads,\n",
    "    # dropout rate applied to the embedding layer\n",
    "    emb_dropout=llm_config.emb_dropout,\n",
    "    # dimensionality of the hidden layer in the feedforward network\n",
    "    ffn_hidden_dim=llm_config.ffn_dim_hidden,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=llm_config.ffn_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|              token_embedding.weight             |  1048576   |\n",
      "|           transformer.0.norm_attn.gain          |    256     |\n",
      "|   transformer.0.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.0.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.0.norm_ffn.gain           |    256     |\n",
      "|    transformer.0.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.0.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.0.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.0.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.1.norm_attn.gain          |    256     |\n",
      "|   transformer.1.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.1.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.1.norm_ffn.gain           |    256     |\n",
      "|    transformer.1.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.1.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.1.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.1.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.2.norm_attn.gain          |    256     |\n",
      "|   transformer.2.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.2.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.2.norm_ffn.gain           |    256     |\n",
      "|    transformer.2.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.2.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.2.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.2.feed_forward.linear2.weight    |   262144   |\n",
      "|           transformer.3.norm_attn.gain          |    256     |\n",
      "|   transformer.3.multihead_attn.proj_qkv.weight  |   196608   |\n",
      "|   transformer.3.multihead_attn.proj_out.weight  |   65536    |\n",
      "|           transformer.3.norm_ffn.gain           |    256     |\n",
      "|    transformer.3.feed_forward.linear1.weight    |   262144   |\n",
      "| transformer.3.feed_forward.swiglu.linear.weight |  2097152   |\n",
      "|  transformer.3.feed_forward.swiglu.linear.bias  |    2048    |\n",
      "|    transformer.3.feed_forward.linear2.weight    |   262144   |\n",
      "|                    norm.gain                    |    256     |\n",
      "|               projection_head.bias              |    4096    |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 12597504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(12597504)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    # Token embedding\n",
    "    params = np.prod(model.token_embedding.weight.shape)\n",
    "    table.add_row([\"token_embedding.weight\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Transformer blocks\n",
    "    for i, block in enumerate(model.transformer_blocks):\n",
    "        # Attention norm\n",
    "        params = np.prod(block.norm_attn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_attn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Multi-head attention\n",
    "        params = np.prod(block.multihead_attn.proj_qkv.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_qkv.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.multihead_attn.proj_out.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_out.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # FFN norm\n",
    "        params = np.prod(block.norm_ffn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_ffn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Feed forward layers\n",
    "        params = np.prod(block.feed_forward.linear1.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear1.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.bias.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.bias\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.linear2.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear2.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "    # Final norm\n",
    "    params = np.prod(model.norm.gain.shape)\n",
    "    table.add_row([\"norm.gain\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Projection head\n",
    "    params = np.prod(model.projection_head.bias.shape)\n",
    "    table.add_row([\"projection_head.bias\", params])\n",
    "    total_params += params\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on METAL.\n",
      "Epoch 1/1:\n",
      "Parameter <Tensor <LB METAL (1, 1, 4096, 1) int ShapeTracker(views=(View(shape=(1, 1, 4096, 1), strides=(0, 0, 1, 0), offset=0, mask=None, contiguous=True),))> on METAL with grad None> has no gradient\n",
      "Parameter <Tensor <LB METAL (128, 128) bool (<BinaryOps.CMPLT: 6>, None)> on METAL with grad None> has no gradient\n",
      "Parameter <Tensor <LB METAL (128, 128) bool (<BinaryOps.CMPLT: 6>, None)> on METAL with grad None> has no gradient\n",
      "Parameter <Tensor <LB METAL (128, 128) bool (<BinaryOps.CMPLT: 6>, None)> on METAL with grad None> has no gradient\n",
      "Parameter <Tensor <LB METAL (128, 128) bool (<BinaryOps.CMPLT: 6>, None)> on METAL with grad None> has no gradient\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[230], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calls 'train' fxn to train the language model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# language model instance to be trained\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# data loader that provides batches of training data\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# learning rate for the optimizer\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max num of epochs to train the model\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# weight decay parameter to prevent overfitting\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# frequency of logging training progress\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch-llm_tinygrad/example/../helpers/trainer.py:58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dl_train, lr, max_epochs, weight_decay, log_every)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Manually apply weight decay\u001b[39;00m\n\u001b[1;32m     61\u001b[0m apply_weight_decay(model, weight_decay)\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch-llm_tinygrad/.venv/lib/python3.12/site-packages/tinygrad/nn/optim.py:34\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m  Performs a single optimization step.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m   Tensor\u001b[38;5;241m.\u001b[39mrealize(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch-llm_tinygrad/.venv/lib/python3.12/site-packages/tinygrad/nn/optim.py:42\u001b[0m, in \u001b[0;36mOptimizer.schedule_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mReturns the tensors that need to be realized to perform a single optimization step.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Tensor\u001b[38;5;241m.\u001b[39mtraining, (\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mTensor.training=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTensor\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Tensor.training must be enabled to use the optimizer.\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m            - help: Consider setting Tensor.training=True before calling Optimizer.step().\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffers\n",
      "File \u001b[0;32m~/Documents/GitHub/scratch-llm_tinygrad/.venv/lib/python3.12/site-packages/tinygrad/nn/optim.py:137\u001b[0m, in \u001b[0;36mLAMB._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2_t \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams):\n\u001b[0;32m--> 137\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm[i]\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm[i] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1) \u001b[38;5;241m*\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m    139\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv[i]\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv[i] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2) \u001b[38;5;241m*\u001b[39m (t\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calls 'train' fxn to train the language model\n",
    "loss_history = train(\n",
    "    # language model instance to be trained\n",
    "    model,\n",
    "    # data loader that provides batches of training data\n",
    "    dl_train,\n",
    "    # learning rate for the optimizer\n",
    "    lr=train_config.learning_rate,\n",
    "    # max num of epochs to train the model\n",
    "    max_epochs=train_config.max_epochs,\n",
    "    # weight decay parameter to prevent overfitting\n",
    "    weight_decay=train_config.weight_decay,\n",
    "    # frequency of logging training progress\n",
    "    log_every=train_config.log_frequency,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new figure and axis object for the plot and sets the size of the figure\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# line plots training loss over time\n",
    "# x-axis: seq of ints from 0 to len of 'train_loss' list minus 1\n",
    "# y-axis: recorded training loss values\n",
    "ax.plot(range(len(loss_history[\"train_loss\"])), loss_history[\"train_loss\"])\n",
    "# sets label for the x-axis \n",
    "ax.set_xlabel(\"step\")\n",
    "# sets label for the y-axis\n",
    "ax.set_ylabel(\"cross entropy loss\")\n",
    "# adds horizontal grid lines to the plot, making it easier to read the y-axis values\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty prompt to generate random stuff\n",
    "# create 2D tensor w/ 1 row and 'llm_config.seq_len' cols, filled w/ the end-of-seq token ID and containing 32-bit ints\n",
    "prompt = torch.full((1, llm_config.seq_len), tokenizer.eos_id, dtype=torch.int32)\n",
    "# moves tensor to specified device for efficient computation\n",
    "prompt = prompt.to(train_config.device)\n",
    "\n",
    "# generates seq of tokens using the model starting from the 'prompt'\n",
    "out = model.generate(prompt, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from a prompt\n",
    "# encodes string into seq of token IDs using the tokenizer\n",
    "prompt = tokenizer.encode(\n",
    "    # prompt text to be encoded \n",
    "    \"KING HENRY VI:\",\n",
    "    # indicates this is the beginning of the string\n",
    "    beg_of_string=True,\n",
    "    # pads the sequence to the specified length\n",
    "    pad_seq=True,\n",
    "    # length to which the sequence should be padded or truncated\n",
    "    seq_len=llm_config.seq_len,\n",
    ")\n",
    "# converts encoded prompt to a PyTorch tensor and moves it to the specified device\n",
    "# 'torch.tensor(prompt, dtype=torch.int32)' converts prompt to a tensor w/ 32-bit int type\n",
    "# '.unsqueeze(0)' adds extra dimension at the beginning, making the tensor shape '(1, seq_len)'\n",
    "# '.to(train_config.device)' moves the tensor to the specified device\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0).to(train_config.device)\n",
    "# generates seq of tokens starting from the given prompt\n",
    "out = model.generate(inputs, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
