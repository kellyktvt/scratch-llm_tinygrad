{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METAL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tinygrad import Tensor, dtypes, Device\n",
    "\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer, train_tokenizer\n",
    "\n",
    "from helpers.dataset import NextTokenPredictionDataset\n",
    "from helpers.trainer import train\n",
    "from helpers.config import LLMConfig, TrainingConfig\n",
    "from helpers.dataloader import DataLoader\n",
    "\n",
    "print(Device.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies achitecture and hyperparameters of the language model\n",
    "llm_config = LLMConfig(\n",
    "    # size of the vocab the model can understand\n",
    "    vocab_size=4096,\n",
    "    # max sequence length of the input tokens; # of tokens model can process in 1 forward pass\n",
    "    seq_len=128,\n",
    "    # dimensionality of embedding vectors; each token in vocab is repped by vector of this size\n",
    "    dim_emb=256,\n",
    "    # num of layers (or transformer blocks) in the model; each consists of sub-layers like self-attention and feedforward\n",
    "    num_layers=4,\n",
    "    # num of attention heads in the multi-head attention mechanism\n",
    "    num_heads=8,\n",
    "    # dropout rate applied to embedding layer to prevent overfitting\n",
    "    emb_dropout=0.0,\n",
    "    # dimensionality of hidden layer in feedforward network; typically a multiple of 'dim-emb'\n",
    "    ffn_dim_hidden=4 * 256,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=False,\n",
    ")\n",
    "\n",
    "# specifies parameters and settings for training the language model\n",
    "train_config = TrainingConfig(\n",
    "    # whether to retrain the tokenizer or not \n",
    "    retrain_tokenizer=False,\n",
    "    # num of samples per batch of training\n",
    "    batch_size=64,\n",
    "    # learning rate for the optimizer; controls how much to adjust the weights w/ respect to the loss gradient\n",
    "    learning_rate=3e-4,\n",
    "    # weight decay parameter; helps prevent overfitting by penalizing large weights\n",
    "    weight_decay=1e-5,\n",
    "    # max number of epochs (full passes thru training dataset) to train the model for\n",
    "    max_epochs=1,\n",
    "    # freq of logging training process (1 = logging after every batch/epoch)\n",
    "    log_frequency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tokenizer and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies path to input text file used for training or retraining tokenizer\n",
    "input_file = \"../data/shakespeare/tinyshakespeare.txt\"\n",
    "# creates new file path for tokenizer model by changing suffix of input file\n",
    "output_file = Path(input_file).with_suffix(\".model\")\n",
    "\n",
    "# checks whether tokenizer model file alr exists or if config specifies to retrain tokenizer\n",
    "if not output_file.exists() or train_config.retrain_tokenizer:\n",
    "    # train tokenizer sing input text file and specified vocab size from 'LLMConfig'\n",
    "    train_tokenizer(input_file, llm_config.vocab_size)\n",
    "\n",
    "# initialize tokenizer by loading it from 'output_file'\n",
    "tokenizer = Tokenizer(str(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Before', '▁we', '▁proceed', '▁any', '▁further', ',', '▁hear', '▁me', '▁speak', '.']\n"
     ]
    }
   ],
   "source": [
    "# defines a string 'sentence' that will be tokenized\n",
    "sentence = \"Before we proceed any further, hear me speak.\"\n",
    "# uses 'EncodeAsPieces' method from 'tokenizer.sp' object to tokenize sentence into tokens and outputs them\n",
    "print(tokenizer.sp.EncodeAsPieces(sentence))\n",
    "\n",
    "# ensures that the encoding and decoding returns the original sentence\n",
    "assert tokenizer.decode(tokenizer.encode(sentence)) == sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128) (64, 128)\n"
     ]
    }
   ],
   "source": [
    "# This helper class allow to generate batches of inputs and targets where targets last element is the next token to predict\n",
    "\n",
    "# initializes instance of 'NextTokenPredictionDataset' class; dataset responsible for generating batches of input sequences and corresponding target sequences for training\n",
    "ds_train = NextTokenPredictionDataset(input_file, llm_config.seq_len, tokenizer)\n",
    "# initializes 'Dataloader' instance that's responsible for creating batches of data from the dataset\n",
    "dl_train = DataLoader(ds_train, batch_size=train_config.batch_size, shuffle=True)\n",
    "\n",
    "# iterates over the batches generates by the data loader\n",
    "for inputs, labels in dl_train:\n",
    "    # prints shapes of 'inputs' and 'labels' tensors; helps verify dimensions of batches\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_resolve_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initializes instance of 'LLM' class; represents the language model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# size of the vocabulary (# of unique tokens)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max sequence length (# of tokens per input sequence)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dimensionality of embeddings vectors\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# number of layers (transformer block) in the model\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# number of attention heads in the multi-head attention mechanism\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dropout rate applied to the embedding layer\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dimensionality of the hidden layer in the feedforward network\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mffn_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_dim_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# whether to include a bias term in the feedforward network layers\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mffn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/example/../model/llm.py:49\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, vocab_size, seq_len, dim_emb, num_layers, attn_num_heads, ffn_hidden_dim, ffn_bias, emb_dropout)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dropout \u001b[38;5;241m=\u001b[39m emb_dropout\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create a list of TransformerBlocks instead of a Sequential object\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_num_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffn_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)\n\u001b[1;32m     51\u001b[0m ]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Initialize RMS normalization for normalizing transformer output w/ 'dim_emb' dimensions\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(dim_emb)\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/example/../model/transformer.py:178\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, seq_len, dim_emb, attn_num_heads, ffn_hidden_dim, ffn_bias, attn_causal)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Follows LLama 2 architecture:\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# - positional encoding on every head of the multi-head attention query and keys projections\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# - RMS pre-normalization instead of layer normalization\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# - SwiGLU activation for the feed__call__\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_attn \u001b[38;5;241m=\u001b[39m RMSNorm(dim_emb)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_num_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_ffn \u001b[38;5;241m=\u001b[39m RMSNorm(dim_emb)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m FeedForward(dim_emb, ffn_hidden_dim, bias\u001b[38;5;241m=\u001b[39mffn_bias)\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/example/../model/transformer.py:90\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, seq_len, num_heads, dim_emb, dim_k, dim_v, causal)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcausal \u001b[38;5;241m=\u001b[39m causal\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# positional encoding to be applied to query and key projections\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# self.positional_encoding = CosinePositionalEncoding(seq_len, dim_emb // num_heads)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mRotaryPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Query, Key and Value projections batched into one linear layer\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_qkv \u001b[38;5;241m=\u001b[39m Linear(dim_emb, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m dim_emb, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/example/../model/transformer.py:27\u001b[0m, in \u001b[0;36mRotaryPositionalEncoding.__init__\u001b[0;34m(self, seq_len, dim_emb, base, eps)\u001b[0m\n\u001b[1;32m     25\u001b[0m position \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m scale\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# , concatenated along the last dimension\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute 'position_cos' and 'position_sin' tensors using cosine and sine fxns applied to 'position'\u001b[39;00m\n\u001b[1;32m     30\u001b[0m position_cos \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mcos(position[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :])  \u001b[38;5;66;03m# (bs, num_heads, seq_len, dim_emb)\u001b[39;00m\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/.venv/lib/python3.12/site-packages/tinygrad/tensor.py:3231\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 3231\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _METADATA\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m TRACEMETA \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3234\u001b[0m     caller_frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(frame \u001b[38;5;241m:=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Wilke/scratch-llm_tinygrad/.venv/lib/python3.12/site-packages/tinygrad/tensor.py:1085\u001b[0m, in \u001b[0;36mTensor.cat\u001b[0;34m(self, dim, *args)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcat\u001b[39m(\u001b[38;5;28mself\u001b[39m:Tensor, \u001b[38;5;241m*\u001b[39margs:Tensor, dim:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m  Concatenates self with other `Tensor` in `args` along an axis specified by `dim`.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m  All tensors must have the same shape except in the concatenating dimension.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m  ```\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m   dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_dim\u001b[49m(dim)\n\u001b[1;32m   1086\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[i] \u001b[38;5;241m==\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m i,s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m dim) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[1;32m   1087\u001b[0m   catargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_resolve_dim'"
     ]
    }
   ],
   "source": [
    "# initializes instance of 'LLM' class; represents the language model\n",
    "model = LLM(\n",
    "    # size of the vocabulary (# of unique tokens)\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    # max sequence length (# of tokens per input sequence)\n",
    "    seq_len=llm_config.seq_len,\n",
    "    # dimensionality of embeddings vectors\n",
    "    dim_emb=llm_config.dim_emb,\n",
    "    # number of layers (transformer block) in the model\n",
    "    num_layers=llm_config.num_layers,\n",
    "    # number of attention heads in the multi-head attention mechanism\n",
    "    attn_num_heads=llm_config.num_heads,\n",
    "    # dropout rate applied to the embedding layer\n",
    "    emb_dropout=llm_config.emb_dropout,\n",
    "    # dimensionality of the hidden layer in the feedforward network\n",
    "    ffn_hidden_dim=llm_config.ffn_dim_hidden,\n",
    "    # whether to include a bias term in the feedforward network layers\n",
    "    ffn_bias=llm_config.ffn_bias,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "\n",
    "    # Token embedding\n",
    "    params = np.prod(model.token_embedding.weight.shape)\n",
    "    table.add_row([\"token_embedding.weight\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Transformer blocks\n",
    "    for i, block in enumerate(model.transformer_blocks):\n",
    "        # Attention norm\n",
    "        params = np.prod(block.norm_attn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_attn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Multi-head attention\n",
    "        params = np.prod(block.multihead_attn.proj_qkv.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_qkv.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.multihead_attn.proj_out.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.multihead_attn.proj_out.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # FFN norm\n",
    "        params = np.prod(block.norm_ffn.gain.shape)\n",
    "        table.add_row([f\"transformer.{i}.norm_ffn.gain\", params])\n",
    "        total_params += params\n",
    "\n",
    "        # Feed forward layers\n",
    "        params = np.prod(block.feed_forward.linear1.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear1.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.swiglu.linear.bias.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.swiglu.linear.bias\", params])\n",
    "        total_params += params\n",
    "\n",
    "        params = np.prod(block.feed_forward.linear2.weight.shape)\n",
    "        table.add_row([f\"transformer.{i}.feed_forward.linear2.weight\", params])\n",
    "        total_params += params\n",
    "\n",
    "    # Final norm\n",
    "    params = np.prod(model.norm.gain.shape)\n",
    "    table.add_row([\"norm.gain\", params])\n",
    "    total_params += params\n",
    "\n",
    "    # Projection head\n",
    "    params = np.prod(model.projection_head.bias.shape)\n",
    "    table.add_row([\"projection_head.bias\", params])\n",
    "    total_params += params\n",
    "\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "# Usage\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls 'train' fxn to train the language model\n",
    "loss_history = train(\n",
    "    # language model instance to be trained\n",
    "    model,\n",
    "    # data loader that provides batches of training data\n",
    "    dl_train,\n",
    "    # learning rate for the optimizer\n",
    "    lr=train_config.learning_rate,\n",
    "    # max num of epochs to train the model\n",
    "    max_epochs=train_config.max_epochs,\n",
    "    # weight decay parameter to prevent overfitting\n",
    "    weight_decay=train_config.weight_decay,\n",
    "    # frequency of logging training progress\n",
    "    log_every=train_config.log_frequency,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new figure and axis object for the plot and sets the size of the figure\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# line plots training loss over time\n",
    "# x-axis: seq of ints from 0 to len of 'train_loss' list minus 1\n",
    "# y-axis: recorded training loss values\n",
    "ax.plot(range(len(loss_history[\"train_loss\"])), loss_history[\"train_loss\"])\n",
    "# sets label for the x-axis \n",
    "ax.set_xlabel(\"step\")\n",
    "# sets label for the y-axis\n",
    "ax.set_ylabel(\"cross entropy loss\")\n",
    "# adds horizontal grid lines to the plot, making it easier to read the y-axis values\n",
    "ax.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty prompt to generate random stuff\n",
    "# create 2D tensor w/ 1 row and 'llm_config.seq_len' cols, filled w/ the end-of-seq token ID and containing 32-bit ints\n",
    "prompt = torch.full((1, llm_config.seq_len), tokenizer.eos_id, dtype=torch.int32)\n",
    "# moves tensor to specified device for efficient computation\n",
    "prompt = prompt.to(train_config.device)\n",
    "\n",
    "# generates seq of tokens using the model starting from the 'prompt'\n",
    "out = model.generate(prompt, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from a prompt\n",
    "# encodes string into seq of token IDs using the tokenizer\n",
    "prompt = tokenizer.encode(\n",
    "    # prompt text to be encoded \n",
    "    \"KING HENRY VI:\",\n",
    "    # indicates this is the beginning of the string\n",
    "    beg_of_string=True,\n",
    "    # pads the sequence to the specified length\n",
    "    pad_seq=True,\n",
    "    # length to which the sequence should be padded or truncated\n",
    "    seq_len=llm_config.seq_len,\n",
    ")\n",
    "# converts encoded prompt to a PyTorch tensor and moves it to the specified device\n",
    "# 'torch.tensor(prompt, dtype=torch.int32)' converts prompt to a tensor w/ 32-bit int type\n",
    "# '.unsqueeze(0)' adds extra dimension at the beginning, making the tensor shape '(1, seq_len)'\n",
    "# '.to(train_config.device)' moves the tensor to the specified device\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0).to(train_config.device)\n",
    "# generates seq of tokens starting from the given prompt\n",
    "out = model.generate(inputs, max_seq_len=64)\n",
    "# decodes generated seq of token IDs back into a human-readable string\n",
    "tokenizer.decode(out.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
